[package]
name = "my-candle-flash-attn"
version = "0.3.0"
edition = "2021"

description = "Flash attention layer for the candle ML framework."
keywords = ["blas", "tensor", "machine-learning"]
categories = ["science"]
license = "MIT OR Apache-2.0"
readme = "README.md"

[dependencies]
#my-candle-core = { path = "../my-candle-core", version = "0.1.1", package = "my-candle-core", features = ["cuda"]}
my-candle-core = { path = "../my-candle-core", version = "0.1.1", package = "my-candle-core"}
half = {version = "2.3.1", features = ["num-traits"]}

[build-dependencies]
anyhow = { version = "1", features = ["backtrace"]}
num_cpus = "1.15.0"
rayon = "1.7.0"

[dev-dependencies]
anyhow = {version = "1", features = ["backtrace"]}
my-candle-nn = { path = "../my-candle-nn", version = "0.1.1" }